#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Stars IndexåŒæ­¥è„šæœ¬ (JSON + Template ç‰ˆ)
åŠŸèƒ½ï¼š
  1. ä» GitHub API æŠ“å–ç”¨æˆ· Star çš„é¡¹ç›®åˆ—è¡¨
  2. å¢é‡è·å– README å¹¶è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œå­˜å‚¨è‡³ JSON æ•°æ®é›†
  3. ä½¿ç”¨ Jinja2 æ¨¡æ¿å°† JSON æ•°æ®æ¸²æŸ“ä¸º Markdown
  4. æ”¯æŒæ¨é€åˆ° Obsidian Vault ä»“åº“
"""

import os
import sys
import json
import time
import base64
import logging
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional
from concurrent.futures import ThreadPoolExecutor

import requests
import yaml
from openai import OpenAI
from jinja2 import Environment, FileSystemLoader
from dotenv import load_dotenv

# åŠ è½½æœ¬åœ° .env æ–‡ä»¶
load_dotenv(override=True)

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = Path(__file__).parent.parent  # ä»“åº“æ ¹ç›®å½•
CONFIG_PATH = SCRIPT_DIR / "config.yml"
DATA_DIR = SCRIPT_DIR / "data"
STARS_JSON_PATH = DATA_DIR / "stars.json"
TEMPLATES_DIR = SCRIPT_DIR / "templates"
DEFAULT_MD_TEMPLATE = "stars.md.j2"
STARS_MD_PATH_DEFAULT = SCRIPT_DIR / "stars.md"

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
DATA_DIR.mkdir(exist_ok=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®åŠ è½½
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_config() -> dict:
    """åŠ è½½é…ç½®ï¼šç¯å¢ƒå˜é‡ä¼˜å…ˆäº config.yml"""
    # æ ¸å¿ƒæ˜ å°„ï¼šç¯å¢ƒå˜é‡å -> (é…ç½®è·¯å¾„, é»˜è®¤å€¼)
    # é…ç½®è·¯å¾„ä½¿ç”¨ç‚¹åˆ†éš”ï¼Œå¦‚ 'ai.model'
    env_mapping = {
        "GH_USERNAME": "github.username",
        "GH_TOKEN": "github.token",
        "GITHUB_TOKEN": "github.token",
        "AI_BASE_URL": "ai.base_url",
        "AI_API_KEY": "ai.api_key",
        "AI_MODEL": "ai.model",
        "MAX_CONCURRENCY": "ai.concurrency",
        "OUTPUT_FILENAME": "output.filename",
        "VAULT_SYNC_ENABLED": "vault_sync.enabled",
        "VAULT_REPO": "vault_sync.repo",
        "VAULT_SYNC_PATH": "vault_sync.path",
        "VAULT_PAT": "vault_sync.pat",
        "PAGES_SYNC_ENABLED": "pages_sync.enabled",
        "TEST_LIMIT": "test_limit",
    }

    # 1. é»˜è®¤åŸºç¡€ç»“æ„
    cfg = {
        "github": {"username": os.environ.get("GH_USERNAME"), "token": None},
        "ai": {
            "model": "gpt-4o-mini",
            "base_url": "https://api.openai.com/v1",
            "api_key": None,
            "concurrency": 5,
        },
        "output": {"filename": "stars"},
        "vault_sync": {
            "enabled": False,
            "repo": None,
            "path": "GitHub-Stars/",
            "pat": None,
            "commit_message": "ğŸ¤– è‡ªåŠ¨æ›´æ–° GitHub Stars æ‘˜è¦",
        },
        "pages_sync": {"enabled": False},
        "test_limit": None,
    }

    # 2. ä» config.yml åŠ è½½ (è‹¥å­˜åœ¨)
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            user_yml = yaml.safe_load(f) or {}
            # è¿™é‡Œç®€å•å¤„ç†ä¸¤å±‚åµŒå¥—
            for section in ["ai", "output", "vault_sync", "pages_sync"]:
                if section in user_yml and isinstance(user_yml[section], dict):
                    cfg[section].update(user_yml[section])

    # 3. ç¯å¢ƒå˜é‡è¦†ç›– (å…·æœ‰æœ€é«˜ä¼˜å…ˆçº§)
    for env_key, config_path in env_mapping.items():
        val = os.environ.get(env_key)
        if val is not None:
            # å¤„ç†ç±»å‹è½¬æ¢
            if env_key in ["MAX_CONCURRENCY", "TEST_LIMIT"]:
                if val.isdigit():
                    val = int(val)
                else:
                    continue
            elif env_key in ["VAULT_SYNC_ENABLED", "PAGES_SYNC_ENABLED"]:
                val = val.lower() == "true"

            # æ›´æ–°åˆ°å­—å…¸
            parts = config_path.split(".")
            target = cfg
            for p in parts[:-1]:
                target = target[p]
            target[parts[-1]] = val

    # 4. å¿…å¡«é¡¹æ ¡éªŒ
    if not cfg["github"]["username"]:
        log.error("âŒ é”™è¯¯: æœªé…ç½® GitHub ç”¨æˆ·å (GH_USERNAME)")
        sys.exit(1)
    if not cfg["ai"]["api_key"]:
        log.error("âŒ é”™è¯¯: æœªé…ç½® AI API Key (AI_API_KEY)")
        sys.exit(1)

    return cfg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®å­˜å‚¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DataStore:
    def __init__(self, path: Path):
        self.path = path
        self.lock = threading.Lock()
        self.data = self._load()

    def _load(self) -> dict:
        if not self.path.exists():
            return {"last_updated": "", "repos": {}}
        try:
            with open(self.path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            log.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return {"last_updated": "", "repos": {}}

    def save(self):
        with self.lock:
            self.data["last_updated"] = datetime.now(timezone.utc).strftime(
                "%Y-%m-%d %H:%M UTC"
            )
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)

    def update_repo(self, full_name: str, metadata: dict, summary: dict):
        with self.lock:
            self.data["repos"][full_name] = {
                "metadata": metadata,
                "summary": summary,
                "pushed_at": metadata.get("updated_at", ""),
                "updated_at": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
            }

    def get_repo(self, full_name: str) -> Optional[dict]:
        return self.data["repos"].get(full_name)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GitHub API å®¢æˆ·ç«¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GitHubClient:
    BASE_URL = "https://api.github.com"

    def __init__(self, username: str, token: Optional[str] = None):
        self.username = username
        self.session = requests.Session()
        self.session.headers.update(
            {
                "Accept": "application/vnd.github+json",
                "X-GitHub-Api-Version": "2022-11-28",
            }
        )
        if token:
            self.session.headers["Authorization"] = f"Bearer {token}"

    def _get(self, url: str, params: dict = None) -> requests.Response:
        for attempt in range(3):
            try:
                resp = self.session.get(url, params=params, timeout=30)
                if resp.status_code == 403 and "rate limit" in resp.text.lower():
                    reset_time = int(
                        resp.headers.get("X-RateLimit-Reset", time.time() + 60)
                    )
                    wait = max(reset_time - int(time.time()), 5)
                    log.warning(f"API é™é€Ÿï¼Œç­‰å¾… {wait} ç§’...")
                    time.sleep(wait)
                    continue
                resp.raise_for_status()
                return resp
            except requests.RequestException as e:
                log.warning(f"è¯·æ±‚å¤±è´¥ï¼ˆç¬¬ {attempt + 1} æ¬¡ï¼‰: {e}")
                time.sleep(2**attempt)
        raise Exception("å¤šæ¬¡è¯·æ±‚å¤±è´¥")

    def get_starred_repos(self) -> list[dict]:
        repos = []
        page = 1
        log.info(f"æ­£åœ¨æŠ“å– @{self.username} çš„ Stars...")
        while True:
            url = f"{self.BASE_URL}/users/{self.username}/starred"
            resp = self._get(
                url,
                params={
                    "per_page": 100,
                    "page": page,
                    "sort": "created",
                    "direction": "desc",
                },
            )
            data = resp.json()
            if not data:
                break
            for item in data:
                repos.append(
                    {
                        "full_name": item["full_name"],
                        "name": item["name"],
                        "owner": item["owner"]["login"],
                        "description": item.get("description") or "",
                        "stars": item["stargazers_count"],
                        "language": item.get("language") or "N/A",
                        "url": item["html_url"],
                        "homepage": item.get("homepage") or "",
                        "topics": item.get("topics", []),
                        "updated_at": item.get("pushed_at", "")[:10],
                    }
                )
            log.info(f"  ç¬¬ {page} é¡µï¼šè·å– {len(data)} ä¸ªï¼Œå…± {len(repos)} ä¸ª")
            if "next" not in resp.headers.get("Link", ""):
                break
            page += 1
        return repos

    def get_readme(self, full_name: str, max_length: int) -> str:
        url = f"{self.BASE_URL}/repos/{full_name}/readme"
        try:
            resp = self._get(url)
            data = resp.json()
            content = base64.b64decode(data["content"]).decode("utf-8", errors="ignore")
            return content[:max_length]
        except Exception:
            return ""

    def push_file(self, repo: str, path: str, content: str, msg: str, pat: str) -> bool:
        url = f"{self.BASE_URL}/repos/{repo}/contents/{path}"
        headers = {
            "Authorization": f"Bearer {pat}",
            "Accept": "application/vnd.github+json",
        }
        sha = None
        try:
            r = requests.get(url, headers=headers, timeout=30)
            if r.status_code == 200:
                sha = r.json().get("sha")
        except Exception:
            pass
        payload = {
            "message": msg,
            "content": base64.b64encode(content.encode("utf-8")).decode("ascii"),
        }
        if sha:
            payload["sha"] = sha
        try:
            r = requests.put(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            log.info(f"âœ… å·²æ¨é€è‡³: {repo}/{path}")
            return True
        except Exception as e:
            log.error(f"âŒ æ¨é€å¤±è´¥: {e}")
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI æ‘˜è¦ç”Ÿæˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class AISummarizer:
    def __init__(
        self, base_url: str, api_key: str, model: str, timeout: int = 60, retry: int = 3
    ):
        self.model = model
        self.retry = retry
        self.client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)

    def summarize(self, repo_name: str, description: str, readme: str) -> dict:
        context = f"Repo: {repo_name}\nDesc: {description}\n\nREADME:\n{readme}"
        prompt = """ä½ æ˜¯ä¸€ä¸ªé¡¶çº§æŠ€æœ¯å¸ƒé“å¸ˆå’Œæ¶æ„å¸ˆã€‚è¯·æ·±å…¥åˆ†æ GitHub ä»“åº“ä¿¡æ¯å¹¶ç”Ÿæˆï¼š
1. **ä¸­æ–‡æ‘˜è¦**ï¼ˆ80-100å­—ï¼‰ï¼šå‡†ç¡®æç‚¼æ ¸å¿ƒä»·å€¼ã€åº”ç”¨åœºæ™¯ä¸æŠ€æœ¯äº®ç‚¹ï¼Œé¿å…ç©ºè¯ã€‚
2. **è‹±æ–‡æ‘˜è¦**ï¼ˆ80-100å­—ï¼‰ã€‚
3. **é«˜æƒé‡å…³é”®è¯æ ‡ç­¾**ï¼ˆä¸­è‹±æ–‡å„ 2-4 ä¸ªï¼‰ï¼š
   - **å®šä½ç²¾å‡†**ï¼šæ ‡ç­¾å¿…é¡»åæ˜ é¡¹ç›®æœ€æ ¸å¿ƒçš„æŠ€æœ¯æ ˆã€é¢†åŸŸåˆ†ç±»æˆ–ç‹¬ç‰¹æ€§ã€‚
   - **æ‹’ç»å¹³åº¸**ï¼šä¸è¦ä½¿ç”¨ "github", "project", "awesome" ç­‰æ— æ„ä¹‰é€šç”¨è¯æ±‡ã€‚
   - **è´¨é‡ä¼˜å…ˆ**ï¼šæ•°é‡ä¸¥æ ¼æ§åˆ¶åœ¨ 2-4 ä¸ªï¼Œå®æ„¿å°‘è€Œç²¾ï¼Œä¸è¦å¤šè€Œæ‚ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{
  "zh": "ä¸­æ–‡æ‘˜è¦",
  "en": "English summary",
  "tags_zh": ["æ ¸å¿ƒæŠ€æœ¯", "ç»†åˆ†é¢†åŸŸ", "ä¸»è¦ç‰¹å¾"],
  "tags_en": ["Core Tech", "Sub-domain", "Key Feature"]
}"""
        for attempt in range(self.retry):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": context},
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"},
                )
                data = json.loads(resp.choices[0].message.content)
                # å…¼å®¹æ—§ç‰ˆæœ¬ç»“æ„
                if "tags" in data and "tags_zh" not in data:
                    data["tags_zh"] = data["tags"]
                return data
            except Exception as e:
                if attempt == self.retry - 1:
                    log.error(f"AI ç”Ÿæˆå¤±è´¥ [{repo_name}]: {e}")
                    return {
                        "zh": "ç”Ÿæˆå¤±è´¥",
                        "en": "Generation failed",
                        "tags_zh": [],
                        "tags_en": [],
                    }
                log.warning(f"AI ç”Ÿæˆå¤±è´¥ [{repo_name}]ï¼Œé‡è¯•ä¸­ {attempt + 1}: {e}")
                time.sleep(2**attempt)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¨¡ç‰ˆç”Ÿæˆå™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TemplateGenerator:
    def __init__(self, template_dir: Path):
        self.env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            trim_blocks=True,
            lstrip_blocks=True,
        )
        # æ·»åŠ ç®€å•çš„ JS è½¬ä¹‰è¿‡æ»¤å™¨
        self.env.filters["escapejs"] = (
            lambda x: x.replace("'", "\\'").replace('"', '\\"').replace("\n", "\\n")
        )

    def render(self, template_name: str, context: dict) -> str:
        template = self.env.get_template(template_name)
        return template.render(context)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    log.info("GitHub Stars IndexåŒæ­¥ç³»ç»Ÿå¼€å§‹è¿è¡Œ")
    cfg = load_config()

    gh = GitHubClient(cfg["github"]["username"], cfg["github"].get("token"))
    ai = AISummarizer(
        cfg["ai"]["base_url"],
        cfg["ai"]["api_key"],
        cfg["ai"]["model"],
        cfg["ai"].get("timeout", 60),
        cfg["ai"].get("max_retries", 3),
    )
    store = DataStore(STARS_JSON_PATH)
    generator = TemplateGenerator(TEMPLATES_DIR)

    # 1. æŠ“å–æ‰€æœ‰ Stars
    all_repos = gh.get_starred_repos()

    # 2. å¢é‡å¤„ç†
    new_repos_to_process = []
    seen_full_names = set()  # é˜²æ­¢ API è¿”å›é‡å¤æ•°æ®
    test_limit = cfg.get("test_limit")

    for repo in all_repos:
        full_name = repo["full_name"]

        # è·³è¿‡å·²ç»åœ¨æ­¤æ¬¡è¿è¡Œä¸­å¤„ç†è¿‡æˆ–å·²å­˜åœ¨äº JSON ä¸­çš„
        if full_name in seen_full_names:
            continue

        existing = store.get_repo(full_name)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†ï¼šå¦‚æœä¸å­˜åœ¨ï¼Œæˆ–è€…æ‘˜è¦æ•°æ®ç¼ºå¤±/æ— æ•ˆ
        is_processed = False
        if existing:
            summ = existing.get("summary", {})
            # åªæœ‰å½“æ‘˜è¦å­˜åœ¨ã€ä¸”ä¸æ˜¯é»˜è®¤çš„å¤±è´¥ä¿¡æ¯æ—¶ï¼Œæ‰è§†ä¸ºå·²å¤„ç†
            if summ and summ.get("zh") and "ç”Ÿæˆå¤±è´¥" not in summ.get("zh"):
                is_processed = True

        if not is_processed:
            if test_limit is not None and len(new_repos_to_process) >= test_limit:
                continue
            new_repos_to_process.append(repo)
            seen_full_names.add(full_name)
        else:
            # æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯ï¼ˆStars æ•°ç­‰ï¼‰ä½†ä¿ç•™å·²æœ‰æ‘˜è¦
            existing["metadata"] = repo
            seen_full_names.add(full_name)

    def process_repo(args):
        idx, repo_data = args
        fname = repo_data["full_name"]
        total = len(new_repos_to_process)

        log.info(f"[{idx}/{total}] æ­£åœ¨å¤„ç†æ–°ä»“åº“: {fname}")
        readme_content = gh.get_readme(fname, cfg["ai"].get("max_readme_length", 4000))

        if not readme_content and not repo_data["description"]:
            summ = {"zh": "æš‚æ— æè¿°ã€‚", "tags": []}
        else:
            summ = ai.summarize(fname, repo_data["description"], readme_content)

        store.update_repo(fname, repo_data, summ)
        return True

    new_count = len(new_repos_to_process)
    if new_count > 0:
        concurrency = cfg["ai"].get("concurrency", 5)
        log.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {new_count} ä¸ªæ–°ä»“åº“ (å¹¶å‘æ•°: {concurrency})")
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            list(executor.map(process_repo, enumerate(new_repos_to_process, 1)))

    if new_count > 0:
        store.save()
        log.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œæ–°å¢ {new_count} æ¡è®°å½•")
    else:
        log.info("âœ¨ æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†")

    # 3. æŒ‰ Star æ—¶é—´é‡æ–°æ’åºï¼ˆæœ€æ–° Star åœ¨å‰ï¼‰
    # JSON é‡Œçš„ repos æ˜¯æ— åºçš„ï¼Œæˆ‘ä»¬æŒ‰ç…§ all_repos çš„é¡ºåºæ¥ç”Ÿæˆï¼ˆå®ƒæ˜¯å€’åºçš„ï¼‰
    ordered_repos = []
    for r_meta in all_repos:
        entry = store.get_repo(r_meta["full_name"])
        if entry:
            # ç¡®ä¿ summary æ ¼å¼æ­£ç¡®ï¼Œé˜²æ­¢æ—§æ•°æ®æˆ–ç©ºæ•°æ®å¯¼è‡´æ¨¡ç‰ˆå´©æºƒ
            summary = entry.get("summary") or {}
            if not isinstance(summary, dict):
                summary = {"zh": str(summary), "tags": []}

            # è¡¥å…¨ç¼ºå¤±å­—æ®µ
            summary.setdefault("zh", "æš‚æ— æ‘˜è¦")
            summary.setdefault("en", summary.get("zh", "No summary available"))
            summary.setdefault("tags_zh", summary.get("tags", []))
            summary.setdefault("tags_en", summary.get("tags", []))

            # åˆå¹¶å±•ç¤ºéœ€è¦çš„æ•°æ®
            view_data = {**entry["metadata"], "summary": summary}
            ordered_repos.append(view_data)

    # 4. ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒ (å–å‰ 5)
    lang_stats = {}
    for r in ordered_repos:
        lang = r.get("language")
        if lang:
            lang_stats[lang] = lang_stats.get(lang, 0) + 1

    # è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨: [{"name": "Python", "count": 10}, ...]
    top_langs = sorted(
        [{"name": k, "count": v} for k, v in lang_stats.items()],
        key=lambda x: x["count"],
        reverse=True,
    )[:5]

    # 5. æ¸²æŸ“ Markdown (å¤šè¯­è¨€ç‰ˆæœ¬)
    context = {
        "last_updated": store.data["last_updated"],
        "repos": ordered_repos,
        "top_langs": top_langs,
    }
    langs = ["zh", "en"]
    generated_mds = {}

    # ç¡®ä¿ dist ç›®å½•å­˜åœ¨
    dist_dir = SCRIPT_DIR / "dist"
    dist_dir.mkdir(exist_ok=True)

    for lang in langs:
        lang_context = {**context, "current_lang": lang}
        base_name = cfg["output"].get("filename", "stars")
        output_name = f"{base_name}_{lang}.md"

        # ç›´æ¥å†™å…¥ dist ç›®å½•
        output_md_path = dist_dir / output_name
        md_content = generator.render(DEFAULT_MD_TEMPLATE, lang_context)

        # ç‰©ç†å†™å…¥ç£ç›˜
        output_md_path.write_text(md_content, encoding="utf-8")

        generated_mds[lang] = {"path": output_md_path, "content": md_content}
        log.info(f"âœ… Markdown ({lang}) ç”Ÿæˆå®Œæˆ: {output_md_path}")

    # 5. å¯é€‰ï¼šVault åŒæ­¥
    v_cfg = cfg.get("vault_sync", {})
    if v_cfg.get("enabled"):
        for lang, data in generated_mds.items():
            # æ‹¼æ¥è·¯å¾„: æ–‡ä»¶å¤¹ + æ–‡ä»¶å + è¯­è¨€ + .md
            vault_dir = v_cfg.get("path", "GitHub-Stars/")
            if not vault_dir.endswith("/"):
                vault_dir += "/"

            base_name = cfg["output"].get("filename", "stars")
            vault_path = f"{vault_dir}{base_name}_{lang}.md"

            gh.push_file(
                v_cfg["repo"],
                vault_path,
                data["content"],
                v_cfg.get("commit_message", "automated update"),
                v_cfg["pat"],
            )

    # 6. å¯é€‰ï¼šGitHub Pages ç”Ÿæˆ
    p_cfg = cfg.get("pages_sync", {})
    if p_cfg.get("enabled"):
        try:
            out_dir = SCRIPT_DIR / p_cfg.get("output_dir", "dist")
            out_dir.mkdir(exist_ok=True)

            html_template = p_cfg.get("template", "index.html.j2")
            html_content = generator.render(html_template, context)

            html_path = out_dir / p_cfg.get("file_name", "index.html")
            html_path.write_text(html_content, encoding="utf-8")
            log.info(f"âœ… HTML ç”Ÿæˆå®Œæˆ: {html_path}")
        except Exception as e:
            log.error(f"âŒ HTML ç”Ÿæˆå¤±è´¥: {e}")

    log.info("åŒæ­¥ä»»åŠ¡ç»“æŸ")


if __name__ == "__main__":
    main()
